<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Kafka</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Kafka</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-分组问题 1 题目 Kafka全流程配置与应用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/28/%E5%88%86%E7%BB%84%E9%97%AE%E9%A2%98%201%20%E9%A2%98%E7%9B%AE%20Kafka%E5%85%A8%E6%B5%81%E7%A8%8B%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%BA%94%E7%94%A8/" class="article-date">
  <time class="dt-published" datetime="2022-05-28T15:51:36.175Z" itemprop="datePublished">2022-05-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>分组问题	1	题目	Kafka全流程配置与应用<br>问题目的	熟练操作Kafka相关配置、并对相应配置流程、应用逻辑进行梳理。<br>问题内容	请各组同学，按以下要求完成一份《Kafka全流程配置与应用》文档。</p>
<p>要求：<br>文档使用word进行编辑（word模板如下）。<br>配置文档需从Kafka环境配置开始，可借鉴课程中的文档进行编写。<br>编写顺序为：<br>1.Kafka环境配置 -&gt;<br>2.Kafka命令行操作 -&gt;<br>3.生产者API -&gt;<br>4.消费者API -&gt;<br>5.Topic管理API<br>问题设计	<br>一、Kafka环境配置<br>1.上传kafka_2.11-2.0.0.tgz到export&#x2F;server，使用tar -zxvf kafka_2.11-2.0.0.tgz<br>命令解压</p>
<p>2.对解压后的Kafka创建软连接方便访问ln -s &#x2F;export&#x2F;server&#x2F;kafka_2.11-2.0.0 &#x2F;export&#x2F;server&#x2F;kafka</p>
<p>3.修改 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;config路径下的配置文件<br>编辑vi server.properties 修改以下内容<br>broker.id&#x3D;0.从0开始,每台不能重复.Listeners &#x3D; plaintext:&#x2F;&#x2F;:9092通信用的是明文传递改成：Listeners &#x3D; plaintext:&#x2F;&#x2F;node1:9092<br>设置数据存储的目录路径log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs<br>默认分区数num.partitions &#x3D; 1<br>指定 zk 集群地址zookeeper.connect&#x3D;node1:2181,node2:2181,node3:2181</p>
<p>4.配置环境变量 &#x2F;etc&#x2F;profile</p>
<p>source &#x2F;etc&#x2F;profile重新加载环境变量<br>5.cd &#x2F;export&#x2F;server分别把配置文件和环境变量的配置文件分发到node2;node3结点上具体命令如下（内容过长不便截图）<br>scp -r &#x2F;export&#x2F;server&#x2F;kafka&#x2F; node2:$PWD<br>scp -r &#x2F;export&#x2F;server&#x2F;kafka&#x2F; node3:$PWD<br>scp -r &#x2F;etc&#x2F;profile&#x2F; node2:etc&#x2F;profile<br>scp -r &#x2F;etc&#x2F;profile&#x2F; node2:etc&#x2F;profile<br>6.修改node2和node3的配置文件<br>&#x2F;export&#x2F;server&#x2F;kafka&#x2F;config&#x2F;server.properties<br>broker.id&#x3D;1<br>listeners &#x3D; plaintext:&#x2F;&#x2F;node2:9092</p>
<p>broker.id&#x3D;2<br>listeners &#x3D; plaintext:&#x2F;&#x2F;node3:9092</p>
<p>7.启动Kafka<br>使用脚本一键启动</p>
<p>二、Kafka命令行操作<br>1.Kafka 中提供了许多命令行工具(位于$KAFKA HOME&#x2F;bin 目录下)用于管理集群的变更。</p>
<p>2.查看当前可用topic<br>.&#x2F;kafka-topics.sh –zookeeper node1:2181,node2:2181,node3:2181 –create –replication-factor 2 –partitions 2 –topic test</p>
<p>3.创建topic<br>bin&#x2F;kafka-topics.sh –create –topic tpc_2 –partitions 2 –replication-factor 2 –zookeeper node1:2181该方式下,命令会自动判断所要创建的 topic 的分区数及副本数</p>
<p>4.删除topic<br>bin&#x2F;kafka-topics.sh  –delete –topic tpc_3 –zookeeper node1：2181<br>删除 topic,需要一个参数处于启用状态: delete.topic.enable &#x3D; true,否则删不掉</p>
<ol start="5">
<li>查看topic<br>kafka-topics.sh  –delete –topic tpc_1 –zookeeper node1:2181</li>
</ol>
<p>6.增加分区数<br>bin&#x2F;kafka-topics.sh –alter –topic tpc_1 –partitions 3 –zookeeper node1:2181<br>需要注意：Kafka 只支持增加分区,不支持减少分区原因是:减少分区,代价太大(数据的转移,日志段拼接合并) 如果真的需要实现此功能,则完全可以重新创建一个分区数较小的主题,然后将现有主题中的消息按照既定的逻辑复制过去;</p>
<p>7.动态配置topic参数<br>添加、修改配置参数bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –add-config compression.type&#x3D;gzip</p>
<p>删除配置参数<br>bin&#x2F;kafka-configs.sh –zookeeper node1:2181 –entity-type topics –entity-name tpc_1 –alter –delete-config compression.type</p>
<p>8.Kafka命令行生产者与消费者操作<br>(1)生产者:kafka-console-producer<br>bin&#x2F;kafka-console-producer.sh –broker-list node1:9092, node2:9092, node3:9092 –topic tpc_1</p>
<p>(2)消费者:kafka-console-consumer<br>1消费消息<br>bin&#x2F;kafka-console-consumer.sh –bootstrap-server node1:9092, node2:9092, node1:9092 –topic tpc_1 –from-beginning(从头开始)</p>
<p>2指定要消费的分区,和要消费的起始 offset<br>bin&#x2F;kafka-console-consumer.sh–bootstrap-servernode1:9092,node2:9092,node3:9092 –topic tcp_1 –offset 2 –partition 0</p>
<p>9.Kafka命令行配置管理<br>bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type topics –entity-name tpc_1 </p>
<p>比如查看 broker 的动态配置可以按如下方式执行:<br>bin&#x2F;kafka-configs.sh zookeeper node1: 2181 –describe –entity-type brokers –entity-name 0 –zookeeper node1:2181</p>
<p>三、生产者API<br>Produce流程图</p>
<p>1.一个正常的生产逻辑需要具备以下几个步骤<br>(1)配置生产者客户端参数及创建相应的生产者实例<br>(2)构建待发送的消息<br>(3)发送消息<br>(4)关闭生产者实例<br>创建一个maven工程<br>并导入相关依赖</p>
<p>采用默认分区方式将消息散列的发送到各个分区当中<br>（代码图片）</p>
<p>Demo详细解释<br>import org.apache.kafka.clients.producer.KafkaProducer;<br>import org.apache.kafka.clients.producer.Producer;<br>import org.apache.kafka.clients.producer.ProducerRecord;<br>import java.util.Properties;<br>public class MyProducer {<br>public static void main(String[ ] args) throws InterruptedException {<br>Properties props &#x3D; new Properties();<br>&#x2F;&#x2F;设置 kafka 集群的地址<br>props.put(“bootstrap.servers”, “node1:9092,node2:9092,node3:9092”);<br>&#x2F;&#x2F;ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，<br>0不等响应就继续发（可靠性低），1leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）<br>props.put(“acks”, “all”);<br>props.put(“retries”, 3); &#x2F;&#x2F;失败重试次数-&gt;失败会自动重试（可恢复&#x2F;不可恢复）–&gt;(有可能会造成数据的乱序)<br>props.put(“batch.size”, 10); &#x2F;&#x2F;数据发送的批次大小提高效率&#x2F;吞吐量太大会数据延迟<br>props.put(“linger.ms”, 10000); &#x2F;&#x2F;消息在缓冲区保留的时间,超过设置的值就会被提交到服务端<br>props.put(“max.request.size”,10); &#x2F;&#x2F;数据发送请求的最大缓存数<br>props.put(“buffer.memory”, 10240); &#x2F;&#x2F;整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端<br>&#x2F;&#x2F;buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性<br>在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。<br>bootstrap.servers<br>key.serializer<br>value.serializer<br>为了防止参数名字符串书写错误,可以使用如下方式进行设置: props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());<br>props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092”);<br>props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());<br>（运行结果图片2）<br>2.生产者api参数发送方式（发后即忘）<br>发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。<br>在大多数情况下,这种发送方式没有问题;<br>不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。<br>这种发送方式的性能最高,可靠性最差。<br>Future<RecordMetadata> send &#x3D; producer.send(rcd);<br>3.同步发送(sync )<br>try {<br>    producer.send(rcd).get();<br>} catch (Exception e) {<br>    e.printStackTrace();<br> }<br>(新版中,producer 在底层只有异步)<br>4.异步发送(async )<br>回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。<br>注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试。</p>
<p>四、消费者API<br>1.一个正常的消费逻辑需要具备以下几个步骤:<br>(1)配置消费者客户端参数<br>(2)创建相应的消费者实例;<br>(3)订阅主题;<br>(4)拉取消息并消费;<br>(5)提交消费位移 offset;<br>(6)关闭消费者实例<br>（代码图片）</p>
<p>五、Topic管理API<br>一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:<br>1.列出主题<br>ListTopicsResult listTopicsResult &#x3D; adminClient.listTopics();<br>Set<String> topics &#x3D; listTopicsResult.names().get();<br>System.out.println(topics);<br>2.查看主题信息<br>DescribeTopicsResultdescribeTopicsResult&#x3D; &#x3D; adminClient.describeTopics(Arrays.asList(“tpc_3”, “tpc_4”));<br>Map&lt;String, TopicDescription&gt; res &#x3D; describeTopicsResult.all().get();<br>Set<String> ksets &#x3D; res.keySet();<br>for (String k : ksets) {<br>    System.out.println(res.get(k));<br>}<br>3.创建主题<br>代码示例:<br>&#x2F;&#x2F; 参数配置<br>Properties props &#x3D; new Properties();<br>props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,”node1:9092,node2:9092,node3:9092”);<br>props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,3000);<br>&#x2F;&#x2F; 创建 admin client 对象<br>AdminClient adminClient &#x3D; KafkaAdminClient.create(props);<br>&#x2F;&#x2F; 由服务端 controller 自行分配分区及副本所在 broker<br>NewTopic tpc_3 &#x3D; new NewTopic(“tpc_3”, 2, (short) 1);<br>&#x2F;&#x2F; 手动指定分区及副本的 broker 分配<br>HashMap&lt;Integer, List<Integer>&gt; replicaAssignments &#x3D; new HashMap&lt;&gt;();<br>&#x2F;&#x2F;分区0分配到broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1));<br>&#x2F;&#x2F; 分区 1,分配到 broker0,broker2<br>replicaAssignments.put(0,Arrays.asList(0,1));<br>NewTopic tpc_4 &#x3D; new NewTopic(“tpc_4”, replicaAssignments);<br>CreateTopicsResultresult&#x3D; &#x3D; adminClient.createTopics(Arrays.asList(tpc_3,tpc_4));<br>&#x2F;&#x2F; 从 future 中等待服务端返回<br>try {<br>    result.all().get();<br>} catch (Exception e) {<br>e.printStackTrace();<br>}<br>adminClient.close();<br>4.删除主题<br>代码示例:<br>DeleteTopicsResultdeleteTopicsResult&#x3D; &#x3D; adminClient.deleteTopics(Arrays.asList(“tpc_1”, “tpc_1”));<br>Map&lt;String, KafkaFuture<Void>&gt; values &#x3D; deleteTopicsResult.values();<br> System.out.println(values);<br>5.除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;</p>
<p>成</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/28/%E5%88%86%E7%BB%84%E9%97%AE%E9%A2%98%201%20%E9%A2%98%E7%9B%AE%20Kafka%E5%85%A8%E6%B5%81%E7%A8%8B%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%BA%94%E7%94%A8/" data-id="cl3q1wz2d0000osv1c4emb0ku" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
  
</article>


</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/05/28/%E5%88%86%E7%BB%84%E9%97%AE%E9%A2%98%201%20%E9%A2%98%E7%9B%AE%20Kafka%E5%85%A8%E6%B5%81%E7%A8%8B%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%BA%94%E7%94%A8/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>